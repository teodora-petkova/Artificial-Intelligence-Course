{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "from skimage.io import imread\n",
    "import scipy.ndimage as im\n",
    "# Write your imports here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing Exercise\n",
    "## Comparing Distributions. Testing statistical hypotheses. p-value. Goodness of fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1. t-test\n",
    "You're given a dataset which describes the age at death among members of the sovereignty, aristocracy, and gentry. There is an explanation [here](http://www.stat.ufl.edu/~winner/data/agedeath.txt) and the actual data is [here](http://www.stat.ufl.edu/~winner/data/agedeath.dat). Get to know the data; plot histograms and / or boxplots, or other graphs as needed. Note that all ages are only for a certain social class of people and they won't represent the entire population of people.\n",
    "\n",
    "Do the mean ages differ significantly ($\\alpha = 5\\%$)? Perform a t-test. Do you need an independent or paired test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>age</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aris</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aris</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aris</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aris</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aris</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category  age  index\n",
       "0     aris   21      1\n",
       "1     aris   21      2\n",
       "2     aris   21      3\n",
       "3     aris   21      4\n",
       "4     aris   21      5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hypothesis\n",
    "# - H0 : all ages are similar\n",
    "# - H1 : ages differ significantly\n",
    "# p-value = 5% (standard)\n",
    "# age is continuous variable\n",
    "\n",
    "#fixed width format\n",
    "age_at_death = pd.read_fwf(\"http://users.stat.ufl.edu/~winner/data/agedeath.dat\", header = None)\n",
    "age_at_death.columns = [\"category\", \"age\", \"index\"]\n",
    "age_at_death.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'age' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a139de35dc99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"age\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# showing the missing data -> a problem with our sample!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'age' is not defined"
     ]
    }
   ],
   "source": [
    "plt.hist(age[\"age\"], bins=20)\n",
    "plt.show()\n",
    "# showing the missing data -> a problem with our sample!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_at_death.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "age_at_death[\"age\"].skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "colors = {\n",
    "    \"aris\" : \"green\",\n",
    "    \"gent\" : \"orange\",\n",
    "    \"sovr\" : \"yellow\"\n",
    "}\n",
    "for category, category_data in age_at_death.groupby(\"category\"):\n",
    "    #print(\"==={}:===\".format(category))\n",
    "    #print(category_data)\n",
    "    #print(\"======\")\n",
    "    plt.hist(category_data[\"age\"], bins=20, alpha=0.7, density=True, \\\n",
    "             label=category, color=colors[category])\n",
    "    plt.axvline(category_data[\"age\"].mean(), color=colors[category])\n",
    "    # histogram in percentage i.e. the values should be comparable!\n",
    "plt.title(\"Age at death for different types of aristocracy\")\n",
    "plt.xlabel(\"Age at death\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_at_death_by_category = {}\n",
    "for category_name in age_at_death[\"category\"].unique():\n",
    "    category_data = age_at_death[age_at_death[\"category\"] == category_name]\n",
    "    age_at_death_by_category[category_name] = category_data[\"age\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.ttest_ind(age_at_death_by_category['aris'], age_at_death_by_category['gent']) # p < 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.ttest_ind(age_at_death_by_category['aris'], age_at_death_by_category['sovr']) # p < 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.ttest_ind(age_at_death_by_category['sovr'], age_at_death_by_category['gent']) # p < 5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "On the basis of the comparision, we assume that there are no differences for the age of deaths for all types of aristocracy ($H_{0}$).\n",
    "We have seen that the differences between the means of the categories are small but we wanted to check whether the difference is significant at all.\n",
    "As a result from the z-test for all the combinations of the three categories, we have seen that the probability to have similar or bigger values for the differences of the means is neglible.\n",
    "\n",
    "Remark: We have seen that the means are representative for the samples. So firstly we need to get acquantited with the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try something else. In the `data/horse_beginners.dat` file, there's data about 8 people. Each person is a beginner in horse riding. They were asked to imagine how long their ride would take (for a fixed track length). After that, the actual times were measured.\n",
    "\n",
    "Get acquainted with the data and draw some plots if you need (hint: boxplots are useful for comparing distributions). Are the people good at predicting their ride times? That is, are there significant differences between imagined and actual time (5%-level)? Also, is the imagined time **significantly longer** than the real time? Perform a t-test. Don't forget that the subjects are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis \n",
    "# - H0 - Imaginary vs Real - NO significant difference of the means  \n",
    "# - H1 - Imaginary vs Real - have a significant difference of the means\n",
    "horse_data = pd.read_fwf('data/horse_beginners.dat', index_col=0)\n",
    "plt.hist(horse_data[\"Actual\"], bins=5, label=\"Actual\")\n",
    "plt.hist(horse_data[\"Imaginary\"], bins=5, label=\"Imaginary\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# the two distribution seem very different but we have to check that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot([horse_data[\"Actual\"], horse_data[\"Imaginary\"]], labels=['Actual', 'Imaginary'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalue = st.ttest_rel(horse_data[\"Actual\"], horse_data[\"Imaginary\"]).pvalue\n",
    "print(\"p-value = %f\" % (pvalue * 100))\n",
    "p_imaginary_smaller_than_actual = pvalue/2 * 100\n",
    "print(\"p(imaginary mean < actual mean) = %f\" % p_imaginary_smaller_than_actual)\n",
    "print(\"p(imaginary mean >= actual mean) = %f\" % (1-p_imaginary_smaller_than_actual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "We have ask the same people and there is a relation between the data, so we can use paired t-test to check whether the samples have identical average values.\n",
    "We have seen that p-value is 0.045 < 0.05, so we can reject H0 that there is no significant difference of the means (i.e. equal averages)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2. ANOVA\n",
    "Read [this](http://www.itl.nist.gov/div898/education/anova/newcar.dat) dataset. Get acquainted with it and plot graphics. Note that it's formatted a bit oddly. First, you have to skip some rows; second, the target variable is first (in most datasets, it's the last one). Well, that shouldn't bother you :).\n",
    "\n",
    "Do the rates differ significantly ($\\alpha = 0,05$) between cities? Apply one-way ANOVA to find out.\n",
    "\n",
    "**Hint:** You have to group all values by city. You can get a dictionary of groups like this:\n",
    "```python\n",
    "groups = { k: v.tolist() for k, v in cars.groupby(\"City\").Rate }\n",
    "```\n",
    "\n",
    "After that, you can use ```*groups.values()``` to pass all dictionary values as separate arguments to a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_interest_rates = pd.read_fwf(\"https://www.itl.nist.gov/div898/education/anova/newcar.dat\", skiprows=25, header=None)\n",
    "car_interest_rates.columns = [\"amount\", \"city\"]\n",
    "car_interest_rates = car_interest_rates[[\"city\", \"amount\"]]\n",
    "plt.hist(car_interest_rates.amount, bins=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city_id, city_data in car_interest_rates.groupby(\"city\"):\n",
    "    print(city_id, len(city_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city_id, city_data in car_interest_rates.groupby(\"city\"):\n",
    "    plt.hist(city_data.amount, bins=5)\n",
    "    plt.xlim(car_interest_rates.amount.min(), car_interest_rates.amount.max())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis:\n",
    "# ask whether there is a significance in the difference in the standard diviation \n",
    "# between the different cities\n",
    "# H0 - there is NO significant difference in the STD\n",
    "# H1 - there is a different in the STD\n",
    "# for example the difference between two cities will be very different if:\n",
    "# - the dispersion for each city is small\n",
    "# - the dispersion between the cities is big (the total deviations)\n",
    "# F = Var(A)city / Var(A)total => F-statistics (Analysis of variance)\n",
    "groups = {k: v.tolist() for k, v in car_interest_rates.groupby(\"city\").amount }\n",
    "\n",
    "st.f_oneway(*groups.values()) # call it with each list as a separate parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "The variations in the groups vs the general variation seem significantly different for the p-value=5%, i.e. again we can reject H0 with p-value 5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, most analyses aren't so simple. Let's have a look at another dataset. This one is located [here](http://www.itl.nist.gov/div898/education/anova/ratfeed.dat). Get acquainted with it.\n",
    "\n",
    "This time, we've got two explanatory variables: amount and type of diet.\n",
    "\n",
    "Perform one-way ANOVA to each variable separately, disregarding the other one. \n",
    "\n",
    "To do this, take for example the \"Amount\" variable. Group all weights by amount, disregarding the diet type. Perform ANOVA on the groups, like you did before. Repeat the same process with \"Diet\".\n",
    "\n",
    "Do the groups differ significantly at the 5%-level?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diet_data = pd.read_fwf(\"https://www.itl.nist.gov/div898/education/anova/ratfeed.dat\", skiprows=25, header=None)\n",
    "diet_data.columns = [\"Weight\", \"Amount\", \"Diet\"]\n",
    "# difference in diets?\n",
    "for diet_type, diet_type_data in diet_data.groupby(\"Diet\"):\n",
    "    plt.hist(diet_type_data.Weight, alpha=0.7, label=diet_type)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference in amount? (high = 1, low  = 2)\n",
    "for diet_amount, diet_amount_data in diet_data.groupby(\"Amount\"):\n",
    "    plt.hist(diet_amount_data.Weight, alpha=0.7, label=diet_amount)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis:\n",
    "# H0 - there is NO significant difference in the STD\n",
    "# H1 - there is a different in the STD\n",
    "groups_by_diet = {k: v.tolist() for k, v in diet_data.groupby(\"Diet\").Weight }\n",
    "groups_by_amount = {k: v.tolist() for k, v in diet_data.groupby(\"Amount\").Weight }\n",
    "print(\"p-value for diet = %f \" % (st.f_oneway(*groups_by_diet.values()).pvalue *100))\n",
    "print(\"p-value for amount = %f \" % (st.f_oneway(*groups_by_amount.values()).pvalue *100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we have to take into account multiple values. In this case, we want to ask another question: Is there a significant difference between combinations of diet and amount? These are called **interacting variables**. Unfortunately, there isn't a quick and easy way to perform this test in `scipy.stats`, but `statsmodels` is another library which will come in handy.\n",
    "\n",
    "We won't go into many details how it works but you basically create a linear model (`ols` stands for \"ordinary least squares\") and you provide a formula. The formula we want to use looks like this:\n",
    "```python\n",
    "formula = \"Weight ~ C(Amount) + C(Diet) + C(Amount):C(Diet)\"\n",
    "```\n",
    "\n",
    "`Weight`, `Amount` and `Diet` are the **column names** (i.e. variable names) in the dataset. `C(Amount)` tells the library to treat the variable as *categorical*, not numerical (e.g. diet 1 + diet 3 makes absolutely no sense). Finally `C(Amount):C(Diet)` means that we take the interaction between these categories (Cartesian product - every amount with every diet).\n",
    "\n",
    "How do we read this formula? \"The Weight column is proportional to the categories Amount, Diet and their interaction\" (i.e. Weight is a linear combination of the three other variables).\n",
    "\n",
    "We are now ready. Well, we could have done all this by hand as well, but why bother :)? Just create the linear model:\n",
    "```python\n",
    "model = ols(formula, rats).fit()\n",
    "```\n",
    "\n",
    "and the ANOVA table:\n",
    "```python\n",
    "table = statsmodels.stats.anova.anova_lm(model, typ = 2)\n",
    "```\n",
    "\n",
    "You can see there's \"type 2\" ANOVA chosen. If you're interested, you can read more about the three types of ANOVA [here](https://mcfromnz.wordpress.com/2011/03/02/anova-type-iiiiii-ss-explained/)\n",
    "\n",
    "P. S.: The type of formulas we described above are typical for another popular language for statistics and data analysis, called **R**. They're also why many people hate R. They're good but need a bit of understanding and getting accustomed to. Case closed :D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the combination of influence between the two variables?\n",
    "\n",
    "formula = \"Weight ~ C(Amount) + C(Diet) + C(Amount):C(Diet)\"\n",
    "model = ols(formula, diet_data).fit()\n",
    "statsmodels.stats.anova.anova_lm(model, typ=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3. Contingency Tables. $\\chi^2$-test\n",
    "Have a look at the dataset `data/Popular Kids.tsv` and its description `data/Popular Kids Description.txt`.\n",
    "\n",
    "When we want to compare data across many categories, a useful way we can visualize it, is using **contingency tables** (or two-way tables). One variable goes to the rows, another one - to the columns of the table. The intersection cell represents the number of observations having this combination of values.\n",
    "\n",
    "Try it now:\n",
    "```python\n",
    "pd.crosstab(kids.Grade, kids.Goals)\n",
    "```\n",
    "\n",
    "You can also specify multiple columns - this will create higher-dimensional tables. Keep in mind, however, that these are difficult to look at and understand. We're usually fine with two variables, three in some rare cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for connections between the grades and the preferences\n",
    "popular_kids = pd.read_table(\"data/Popular Kids.tsv\")\n",
    "#popular_kids[(popular_kids.Grade == 4) & (popular_kids.Goals == \"Popular\")]\n",
    "grades_and_goals = pd.crosstab(popular_kids.Grade, popular_kids.Goals)\n",
    "grades_and_goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **$\\chi^2$**-test provides a way to test for association between two (or more) categorical variables. In this case, **Grade** and **Goals** seem to be good candidates. This is most easily done using the contingency table. Fortunately, `scipy.stats` has one such method (read the docs [here](https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.stats.chi2_contingency.html)).\n",
    "\n",
    "Note that this test doesn't specify what the relationship is. It just tells us that there might be a significant relationship.\n",
    "\n",
    "Are goals related to grade? To what extent? Are these significant?\n",
    "* $H_0$: No relation between Grade and Goals\n",
    "* $H_1$: Some relation exists between Grade and Goals\n",
    "* $\\alpha = 0,05$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.chi2_contingency(grades_and_goals) # chi-square test of independence of variables in a contingency table\n",
    "# we cannot reject Ho as 0.86 > 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The school areas are also divided into \"Urban\", \"Suburban\" and \"Rural\". Are the school areas related to goals? Perform the same type of test. Is the difference significant now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to continue exploring the dataset as you wish. You can find interesting data and relationships. \n",
    "\n",
    "**Note:** You'll see the catch of categorical variables - these are very difficult to visualize properly. Most plots just appear as symmetrical patterns of dots. You can rely on contingency tables and correlations to properly describe and explore these variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Problem 4. Image Convolution\n",
    "\"Convolution\" refers to a process similar to \"mixing\" two functions. It can be performed in one dimension (e.g. on audio data), or many dimensions. In this problem, we'll look at 2D convolution of images and what we can do with it.\n",
    "\n",
    "Let's first read an image. Once again, to make things easier, we'll separate the channels. We can work on all three channels separately or at once but it's easier to work with one channel only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image = imread(\"https://upload.wikimedia.org/wikipedia/commons/d/d9/Norwegian_Forest_Cat_Portrait.JPG\")\n",
    "\n",
    "def display(image):\n",
    "    # If there is only one channel to show, display it as grayscale\n",
    "    cm = None\n",
    "    if(len(image.shape)) == 2:\n",
    "        cm = \"gray\"\n",
    "    plt.figure(figsize = (5, 10))\n",
    "    plt.imshow(image, cmap = cm)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n",
    "\n",
    "display(original_image)\n",
    "red_channel = original_image[:, :, 0]\n",
    "display(red_channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolution** means taking a special square matrix (usually 3x3 or 5x5), called a **convolution kernel** and applying it to the image like this: the central pixel of the resulting image is the sum of element-wise products between the image and the kernel:\n",
    "![Convolution example](convolution.png)\n",
    "\n",
    "After that, the kernel moves 1px to the right and contiinues. It \"slides\" across the entire image. The edge pixels are a bit problematic but there are several ways to deal with that. The most common way is to copy whatever pixel value happened to be at the border.\n",
    "\n",
    "![Border handling](border.png)\n",
    "\n",
    "The algorithm is always the same. The output depends on the **kernel**. Different kernels produce different results: some detect edges (lines), others detect corners; some apply blurring and sharpening; some remove noise, etc.\n",
    "\n",
    "The results can be useful for analyzing what's on the image, or just for artistic purposes.\n",
    "\n",
    "Let's examine this filter, for example:\n",
    "\n",
    "$$ F = \\begin{bmatrix}\n",
    "1/9 & 1/9 & 1/9 \\\\\n",
    "1/9 & 1/9 & 1/9 \\\\\n",
    "1/9 & 1/9 & 1/9\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This calculates the average of all surrounding pixels and basically smooths the image.\n",
    "\n",
    "Note that in order to preserve brightness, the sum of all elements in $F$ must be equal to 1. If it's not, the image will be darker or brighter (which may or may not be a desired effect).\n",
    "\n",
    "`scipy.ndimage` has a method for performing 1D and multi-dimensional convolution. Read the docs [here](https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.ndimage.filters.convolve.html#scipy.ndimage.filters.convolve).\n",
    "\n",
    "Apply the convolution. To see better how it performs, you can plot only a part of the image - this will zoom the entire thing. Compare the \"before\" and \"after\" images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.ones((3, 3)) * (1/9)\n",
    "\n",
    "smoothed_image = im.convolve(red_channel, kernel)\n",
    "\n",
    "display(red_channel[200:350, 100:300])\n",
    "display(smoothed_image[200:350, 100:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play around with more kernels (they're also called **filters**). You can find examples on the Internet, or you can create your own. Have fun :).\n",
    "\n",
    "Try these filters:\n",
    "* Gaussian blur\n",
    "* Sobel edge detector - vertical, horizontal\n",
    "* Corner detector\n",
    "* Gradient detector\n",
    "* Sharpening\n",
    "* Unsharp mask\n",
    "\n",
    "For each filter, show the result before and after its application.\n",
    "\n",
    "Sources: <a href=\"https://en.wikipedia.org/wiki/Kernel_(image_processing)\">Wikipedia</a>, [online playground](http://matlabtricks.com/post-5/3x3-convolution-kernels-with-online-demo), [Image Kernels explained visually](http://setosa.io/ev/image-kernels/).\n",
    "\n",
    "**Optional:** Think about you might use edge, corner and gradient detectors in image processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Problem 5. Classification\n",
    "A type of widely used **models** is **classification**. Regression outputs a continuous value while classification outputs one of several pre-defined classes. In the most simple way, the classes are only two. For example, if we want to detect whether there's a cat on an image, we can have two classes: \"cat\" and \"non-cat\".\n",
    "\n",
    "Explore the problem of classification. Implement and document one algorithm. Apply it to some real-world data. You can use the following checklist:\n",
    "\n",
    "**Note:** If your paper is **about the algorithm**, consider **writing it from scratch**, not reusing it from a library.\n",
    "\n",
    "* What is supervised learning? What do supervised learning models do?\n",
    "* What is regression? What is classification?\n",
    "* What types of problems does classification solve directly?\n",
    "    * What types of problems can be reduced to classification?\n",
    "* What's the difference between two-class and multi-class classification?\n",
    "* Explore one algorithm for classification, e.g. logistic regression.\n",
    "    * State the problem clearly\n",
    "    * List all sssumptions of the modelling function\n",
    "    * Describe the process: distances, error function, total loss, gradient descent, etc.; as needed\n",
    "    * Implement the algorithm from scratch\n",
    "* Select or generate a small dataset, suitable for classification. Run your algorithm as a sanity check\n",
    "* Debug and solve any problems\n",
    "* Waht is a confusion matrix?\n",
    "* What metrics are used to score a classifier?\n",
    "    * Accuracy, Precision, Recall, others\n",
    "    * ROC curve, interpretation\n",
    "* Select a real dataset\n",
    "    * Explore it to get acquainted with what information it contains\n",
    "    * Clean up the data if you need to do so\n",
    "    * Perform classification\n",
    "    * Score your classification model\n",
    "    * Use your classifier to predict\n",
    "        * Split the data into training and testing set\n",
    "        * Optionally, perform **cross-validation**\n",
    "    * Compare your implementation to another one, e.g. `scikit-learn`. They should give the same (or very similar) results\n",
    "    * Communicate the results on your dataset\n",
    "    * Optionally, publish your model on the Internet as a Web API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Problem 6. Fourier Transform Applications: Images\n",
    "Examine how Fourier transform is used in image processing. Consider writing the algorithms for transformation (forward and inverse) from scratch.\n",
    "\n",
    "You may look at other transformations as well, for example, the **cosine transform** (used in JPEG images to achieve compression) or **wavelet transform**. You can use the following checklist:\n",
    "\n",
    "* What is Fourier transformation? What information does it provide?\n",
    "    * What do the forward and inverse transformation do? What are their inputs and outputs?\n",
    "    * How do we interpret the results (\"time domain\" / \"frequency domain\")?\n",
    "        * What does this mean in 2D? How does a 1D spectrum differ from a 2D spectrum?\n",
    "        * What are the characteristic parts of a 2D spectrum?\n",
    "    * Give some examples of well-known functions and their transformations\n",
    "* What is Discrete Fourier Transform (DFT)?\n",
    "    * Implement the algorithm in 2D\n",
    "* How do we use DFT in image processing?\n",
    "    * Provide examples\n",
    "* Select an image and perform operations on it using your implementation of DFT\n",
    "    * Low-pass filtering\n",
    "    * High-pass filtering\n",
    "    * Finding and removal of periodic component from an image\n",
    "    * Fnding and removal of peaks\n",
    "    * Shape analysis\n",
    "* How do convolutions relate to DFT?\n",
    "    \n",
    "[This document](http://www.robots.ox.ac.uk/~az/lectures/ia/lect2.pdf), [this document](http://research.stowers.org/mcm/efg/Report/FourierAnalysis.pdf), and many others provide a good starting point. You can find a lot of examples in digital image processing and art using Fourier transformations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
