{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz 3 #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q.1. Decision Trees ###\n",
    "Decision trees cannot be used for regression problems.\n",
    "False\n",
    "There are regression trees whose leaves represent numeric values.\n",
    "Classification trees have True or False as their leaves or some other discrete category.\n",
    "* For regression trees we split the data by a feature into two groups and find the threshold that gives the smalles sum of squared residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q.2. Decision Tree Splitting\n",
    "Which of the following is / are correct? When we construct a decision tree, we split the features based on the:\n",
    "* mean squared error\n",
    "* logistic decision function\n",
    "* Manhattan distance between any two features\n",
    "* gini coefficient - YES\n",
    "* information gain - YES\n",
    "* entropy - YES\n",
    "* cross-entropy\n",
    "\n",
    "Notes:\n",
    "\n",
    "* gini index - it measures how often a randomly chosen element from the set would be incorrectly labeled\n",
    "J classes, suppose $ i\\in \\{1,2,...,J\\} $ \n",
    "$ {I}_{G}(p) = 1-\\sum_{i=1}^{J}{p_{i}}^{2} $\n",
    "* entropy - the average level of \"information\", \"surprise\", or \"uncertainty\" inherent in the variable's possible outcomes; it gauges the disorder of a grouping by the target variable but uses log instead of probabilities only\n",
    "$ H(X)=-\\sum_{i=1}^{n}\\mathrm{P} (x_{i})\\log \\mathrm {P} (x_{i}) $\n",
    "* Information gain - the decrease in entropy after a dataset is split on a feature/an attribute; once we derive the impurity of the dataset, we can see how much information is gained as we go down the tree and measure the impurity of the nodes; choose the feature with the highest information gain - it is the most relevant feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q.3. Categorical Data ###\n",
    "In real-life scenarios, your data will consist of real-valued (i.e. numerical) variables and categorical variables. Suppose you convert the categorical variables to integers (0, 1, 2, etc.). Which of the algorithms will work correctly with the categorical variables, without additional settings?\n",
    "\n",
    "* Logistic regression\n",
    "* Decision tree\n",
    "* Random forest\n",
    "* AdaBoost\n",
    "* None of the above -> TRUE\n",
    "\n",
    "Note:\n",
    "When we just convert the values to numeric values, we can compare them and they should not be compared! So we can use:\n",
    "* the one-hot encoding scheme - It encodes or transforms the attribute into m binary features which can only contain a value of 1 or 0. Each observation in the categorical feature is thus converted into a vector of size m with only one of the values as 1 (indicating it as active)\n",
    "* the dummy encoding scheme - It is similar to the one-hot encoding scheme, except in the case of dummy coding scheme, when applied on a categorical feature with m distinct labels, we get m - 1 binary features. The extra feature is completely disregarded and thus if the category values range from {0, 1, …, m-1} the 0th or the m-1th feature column is dropped and corresponding category values are usually represented by a vector of all zeros (0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q.4. Performance ###\n",
    "You want to perform model selection on a random forest classifier. To do this, you perform grid search on two parameters: number of trees (k): 100, 200, 300; learning rate (alpha): 0.01, 1, 10. Which parameter will not influence the running time of the training during cross-validation? Write \"k\" or \"alpha\".\n",
    "\n",
    "\"alpha\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q.5. Metrics ###\n",
    "You want to train a random forest to classify images of cancerous cells (positive, class 1) vs. healthy cells (negative, class 0). In your dataset, you have 50 000 samples in total, 5000 of which are positive. Which measure is the most accurate to measure the performance of a model using AdaBoost? Note that while all measures will give somewhat valid results, there is only one correct answer.\n",
    "* recall -> YES - it is important not to omit positives R = TP / (TP + FN)\n",
    "* F statistic\n",
    "* precision\n",
    "* accuracy\n",
    "* R^2 score\n",
    "* F1 measure\n",
    "* gradient descent\n",
    "* entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q.6. More Data ###\n",
    "You want to train a decision forest to classify images of cats and dogs. There are approximately the same number of pictures of cats and dogs in your dataset. You fix all parameters (500 trees, depth = 5) and don't perform any hyperparameter tuning. You train an algorithm on 30% of the data, and another algorithm - on 80%. Which algorithm is more likely to overfit?\n",
    "* first (30% of the train data) -> TRUE\n",
    "* second (80% of the train data)\n",
    "\n",
    "In the general case, adding more data decreases the high variance and overfitting, of course if we do not add more noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q.7. Bagging Classifier ###\n",
    "Suppose you want to use a BaggingClassifier to train a model. This classifier works similarly to AdaBoost. You can look at the docs for more information. You want to combine 100 estimators. Each individual estimator has approximately 60% accuracy. What is the minimum accuracy that you will get using the BaggingClassifier?\n",
    "*  < 60%\n",
    "*  /> 60% -> TRUE\n",
    "*  = 60%\n",
    "* Half of the accuracy of the weakest learner, about 30%\n",
    "* Twice the accuracy of the weakest learner, so it will be 100%\n",
    "\n",
    "Note:\n",
    "Bagging Classifier\n",
    " * we create multiple bootstrap samples so that each new bootstrap sample will act as another (almost) independent dataset drawn from true distribution\n",
    " * then, we can fit a weak learner for each of these samples \n",
    " * finally we aggregate them such that we kind of “average” their outputs and, so, obtain an ensemble model with less variance that its components\n",
    " \n",
    "“Averaging” weak learners outputs do not change the expected answer but reduce its variance (just like averaging i.i.d. random variables preserve expected value but reduce variance)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
