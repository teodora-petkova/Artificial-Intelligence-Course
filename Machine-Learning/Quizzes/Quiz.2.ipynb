{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 2 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q.1. Model Diagnostics###\n",
    "Suppose you train a model to predict temperature for a given day (regression). The model performs somewhat well on the training data but not enough. Its performance on the testing data is slightly better but still far from what you desire. What is the problem most likely to be?\n",
    "\n",
    "* The model is not doing well in general -> high bias\n",
    "\n",
    "* The model is better on the testing set, so it performs well on a specific group of data present in the test data and overfitted that group -> high variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q.2. Regularization ###\n",
    "You've got a dataset with two features. You train two logistic regression models: model1 (C = 100), and model2 (C = 1). One model returns the following weights: [31.29, 28.14], the other's weights are [1.12, 3.46]. Which model is the SECOND set of weights more likely to correspond to and why?\n",
    "\n",
    "The second weights are smaller, so we have high regularization.\n",
    "Lambda is high and C is low. So...\n",
    "\n",
    "* model2 because it has low regularization\n",
    "* model2 because it has high regularization   -> True\n",
    "* model1 because it has low regularization\n",
    "* model1 because it has high regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data, iris_types = load_iris().data, load_iris().target\n",
    "iris_data_scaled = MinMaxScaler().fit_transform(iris_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.16296292e-08,  7.72222196e-09, -1.94576265e-08,\n",
       "        -1.98611104e-08],\n",
       "       [ 1.28703699e-09, -5.98611091e-09,  4.25423714e-09,\n",
       "         2.63888880e-09],\n",
       "       [ 1.03425922e-08, -1.73611105e-09,  1.52033893e-08,\n",
       "         1.72222216e-08]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# small C -> smaller weights -> bigger lambda -> bigger regularization \n",
    "# -> we do NOT care so much about the data\n",
    "iris_model1 = LogisticRegression(C = 0.01)\n",
    "iris_model1.fit(iris_data_scaled, iris_types)\n",
    "iris_model1.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-18.50439934,  29.40165499, -38.2104263 , -38.4367227 ],\n",
       "       [ 13.69282683,  -6.68702161,  -8.71329212,  -2.71987842],\n",
       "       [  4.81157251, -22.71463338,  46.92371842,  41.15660112]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# big C -> larger weights -> small lambda -> bigger regularization \n",
    "# -> we care about the data!\n",
    "iris_model2 = LogisticRegression(C = 100)\n",
    "iris_model2.fit(iris_data_scaled, iris_types)\n",
    "iris_model2.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q.3. Regularization Coefficient  ###\n",
    "Which of the following are true about the regularization coefficient lambda?\n",
    "\n",
    "* Using a value that is too large may lead to overfitting  -> FALSE\n",
    "* Using a value that is too small will make the algorithm converge very slowly\n",
    "* Using a value that is too small may lead to underfitting -> FALSE\n",
    "* Using a value that is too large may lead to underfitting -> TRUE\n",
    "* Using a value that is too small may lead to overfitting -> TRUE\n",
    "* Using a value that is too large will make the algorithm \"miss\" the minimum\n",
    "* The value is not related to underfitting or overfitting; it's there to speed up the algorithm convergence\n",
    "* Using a value that is too small will make the algorithm \"miss\" the minimum\n",
    "* Using a value that is too large will make the algorithm converge very slowly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q.4. Bias and Variance ###\n",
    "Which of the following are true?\n",
    "\n",
    "* Plotting a learning curve is not enough to diagnose high bias or high variance -> True\n",
    "* If an algorithm suffers from high variance, adding more features will lower the variance significantly -> False (it is about high bias, more features help to get more important patterns into consideration and increase accuracy)\n",
    "* If an algorithm suffers from high bias, adding more examples will lower the bias significantly -> False (it is about high variance, more examples, will lower the noise for the model; for high variance also it will help to have less features)\n",
    "* When an algorithm has much lower training set error than test set error, it suffers from high variance -> True (the algorithm has learned too well the training set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q.5. Classification Metrics ###\n",
    "You want to train a model to recognize spam messages. The data you've got contains 92% non-spam messages (class 0, negative) and 8% spam messages (class 1, positive). Which of the following are true?\n",
    "\n",
    "So, for a model that always outputs class 0, accuracy is 92% /\n",
    "                                             precision is 0% /\n",
    "                                             recall is 0% /\n",
    "                                            \n",
    "And for a model that always outputs class 1, accuracy is 8% /\n",
    "                                             precision is 8% /\n",
    "                                             recall is 100% /\n",
    "     \n",
    "\n",
    "* For a model that always outputs class 1, the precision is is 8%       -> True\n",
    "* For a model that always outputs class 1, the precision is is 100%     -> False\n",
    "* For a model that always outputs class 0, the accuracy is 92%          -> True\n",
    "* For a model that always outputs class 0, the recall is is 100%        -> False\n",
    "* For a model that always outputs class 0, the accuracy is 8%           -> False\n",
    "* For a model that always outputs class 1, the accuracy is 100%         -> False\n",
    "* For a model that always outputs class 0, the recall is is 92%         -> False\n",
    "* For a model that always outputs class 1, the precision is is 0%       -> False\n",
    "* For a model that always outputs class 0, the recall is is 0%          -> True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_y = []\n",
    "for i in range(10000):\n",
    "    real_y.append(np.random.choice(np.arange(0,2), p = [0.92, 0.08]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96      9185\n",
      "           1       0.00      0.00      0.00       815\n",
      "\n",
      "    accuracy                           0.92     10000\n",
      "   macro avg       0.46      0.50      0.48     10000\n",
      "weighted avg       0.84      0.92      0.88     10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lubuntu/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# the model always outputs class 0\n",
    "predicted_y = [0 for i in range(10000)]\n",
    "#print(confusion_matrix(real_y, predicted_y))\n",
    "print(classification_report(real_y, predicted_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      9185\n",
      "           1       0.08      1.00      0.15       815\n",
      "\n",
      "    accuracy                           0.08     10000\n",
      "   macro avg       0.04      0.50      0.08     10000\n",
      "weighted avg       0.01      0.08      0.01     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the model always outputs class 1\n",
    "predicted_y = [1 for i in range(10000)]\n",
    "#print(confusion_matrix(real_y, predicted_y))\n",
    "print(classification_report(real_y, predicted_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
