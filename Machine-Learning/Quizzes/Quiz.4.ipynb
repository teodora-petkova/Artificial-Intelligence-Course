{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz 4#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q.1. Support Vectors ###\n",
    "Which of the following best describes support vectors?\n",
    "\n",
    "* A linear combination of the input features\n",
    "* A linear combination of the input samples (observations)\n",
    "* All data points\n",
    "* Data points that lie closest to the decision surface\n",
    "* Data points that lie furthest away from the decision surface -> TRUE\n",
    "* Features with the highest importance\n",
    "* Features with the least importance (aka those which we could afford to get rid of)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q.2. Linear SVM Classifier ###\n",
    "True or false? For a linearly separable dataset with two classes, logistic regression and linear SVM will produce the same decision boundary.\n",
    "* False \n",
    "\n",
    "The logistic regression tries to adapt a linear regression, so that it estimates the probability a new entry falls in a class.  It measures the relationship between the categorical dependent variable and one or more independent variables by estimating probabilities using a logistic/sigmoid function. The linear decision boundary is simply a consequence of the structure of the regression function and the use of a threshold in the function to classify.\n",
    "\n",
    "The decision boundary is much more important for Linear SVM's - the maximum margin between the support vectors, no probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q.3. SVM Accuracy ###\n",
    "You train a linear SVM classifier on three features. It learns the decision boundary given by the plane 3x_1 - 2x_2 + 5x_3 = 0. The table below represents your testing data. What is the accuracy of your model on this data?\n",
    "60%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification accuracy is 60%.\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.DataFrame(np.array([\n",
    "         [-5, 2, 1, 0], \n",
    "         [3, 17, 11, 0], \n",
    "         [16, 18, -2, 1],\n",
    "         [4, -2, -9, 0],\n",
    "         [-5, 4, -10, 1]]),\n",
    "        columns = ['x1', 'x2', 'x3', 'y'])\n",
    "test_features = test_data.drop('y', axis = 1)\n",
    "test_labels = test_data['y']\n",
    "\n",
    "model_boundary = lambda x1, x2, x3: 3 *x1 - 2 * x2 + 5 * x3\n",
    "model_predicts = [1 if model_boundary(x1, x2, x3) >= 0 else 0 for x1, x2, x3  in test_features.values]\n",
    "\n",
    "count_TP_TN = np.sum(np.array(model_predicts) == np.array(test_labels))\n",
    "count_all = len(test_features)\n",
    "print(f\"The classification accuracy is {((count_TP_TN / count_all) * 100).astype(int)}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q.4. RBF Kernel ###\n",
    "Given a training set with 3 features and 10 examples, you train an SVM with a Gaussian (RBF) kernel. How many dimensions does the kernel function \"project\" the data into?\n",
    "\n",
    "* 3 + 1 = 4\n",
    "\n",
    "Cover's theorem states that given a set of training data that is not linearly separable, one can with high probability transform it into a training set that is linearly separable by projecting it into a higher-dimensional space via some non-linear transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q.5. Hyperparameters\n",
    "Which of the following statements about SVMs is / are true?\n",
    "* A polynomial kernel can accept fractional degrees (e.g. 1.5) which allows us to use roots (square root, etc.) of feature columns -> TRUE (tested with sklearn)\n",
    "* Like logistic regression, an SVM classifier outputs the probability that each sample belongs to a certain class -> FALSE\n",
    "* A polynomial kernel can be of any degree > 1\n",
    "* Using a linearly separable dataset, a gaussian SVM will always lead to overfitting\n",
    "* Decreasing C leads to more training error -> TRUE\n",
    "* A polynomial kernel can be at most of degree 3 (cubic) -> FALSE\n",
    "* Increasing C leads to more training error -> FALSE\n",
    "* Decreasing C leads to a \"wider margin\" -> TRUE\n",
    "* Increasing C leads to a \"wider margin\" -> FALSE\n",
    "\n",
    "Notes:\n",
    "\n",
    "From lectures: C is the penalty for misclassification ($C = \\frac{1}{\\lambda}$); smaller value = less strict and less regularizaion.\n",
    "\n",
    "The C parameter controls how much you want to punish your model for each misclassified point for a given curve:\n",
    "\n",
    "| Large values of C | Small Values of C|\n",
    "|------|------|\n",
    "| Large effect of noisy points. |Low effect of noisy points.|\n",
    "| A plane with very few misclassifications will be given precedence | Planes that separate the points well will be found, even if there are some misclassifications\n",
    "| less training error | more training error |\n",
    "| narrower margin | wider margin |\n",
    "| more strict | less strict |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q.6. k-Nearest Neighbors ###\n",
    "Which of the following is / are true about kNN?\n",
    "* Decreasing k leads to higher bias - TRUE (local overfitting)\n",
    "* kNN is much more computationaly expensive to train than to predict new data points - FALSE\n",
    "* kNN can only be used for clustering - FALSE\n",
    "* kNN can be used for value imputation - TRUE\n",
    "* kNN can only describe the training set; it cannot predict new data points - FALSE\n",
    "* k has to be a number strictly greater than 1 - FALSE (we can have kNN with k = 1)\n",
    "* kNN can only be used for classification - FALSE\n",
    "\n",
    "Note:\n",
    "kNN can be used for classification, regression, Voronoi tiling/tesselation etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q.7. Anomaly Detection ###\n",
    "True or false? When training a learning algorithm to perform anomaly detection, we need at least one observation of the anomalous class.\n",
    "* True\n",
    "* False - TRUE\n",
    "* Anomaly detection can never be a supervised learning task\n",
    "* True only if we perform feature selection; false otherwise\n",
    "* True only if we use an ensemble of algorithms (e.g. random forest, AdaBoost); false otherwise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
