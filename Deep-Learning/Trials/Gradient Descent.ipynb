{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is an optimization technique that can find the minimum of an objective function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 20)\n",
    "y = 2 * x + 3\n",
    "y_noise = np.random.normal(loc = 0, scale = 2.5, size = len(x))\n",
    "y += y_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x, y)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent with Logistic Regression - basic line\n",
    "\n",
    "We want to find a line that will describe the connection between our two variables - x and y. We make an assumption that a linear modeling function will be a good enough model to describe them.\n",
    "\n",
    "Equation of a line: $y = ax + b$\n",
    "* a - the slope $a = \\frac{y_2 - y_1}{x_2 - x_1} = \\frac{change in y}{change in x}$\n",
    "* b - y intercept\n",
    "\n",
    "m - the number of the training examples\n",
    "n = 2 - the number of the features - a, b => two weights/thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_gradient_descent(x, y, a_guess, b_guess, learning_rate, max_iterations):\n",
    "    def perform_single_step(x, y, a, b, learning_rate):\n",
    "        m = len(x)\n",
    "        dJ_da = - 2/m * np.sum(x * (y - a * x - b))\n",
    "        dJ_db = - 2/m * np.sum(y - a * x - b)\n",
    "        \n",
    "        a_new = a - learning_rate * dJ_da\n",
    "        b_new = b - learning_rate * dJ_db\n",
    "                               \n",
    "        return(a_new, b_new)\n",
    "    \n",
    "    for step in range(max_iterations):\n",
    "        a_guess, b_guess = perform_single_step(x, y,\n",
    "                                               a_guess, b_guess,\n",
    "                                               learning_rate)\n",
    "    return (b_guess, a_guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "number_iterations = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m and b: (2.5717536257265983, 1.527067402482388)\n"
     ]
    }
   ],
   "source": [
    "line_parameters = linear_regression_gradient_descent(x, y, -10, 20, learning_rate, number_iterations)\n",
    "print(\"m and b:\", line_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent - general case in more dimensions\n",
    "\n",
    "<b>Parameters:</b>\n",
    "\n",
    "* Input - x -> a matrix m x n (m training examples with n features)\n",
    "* Output - y -> m vector\n",
    "* Weights - thetas -> n\n",
    "\n",
    "<b>Functions:</b>\n",
    "* Hypothesis function:\n",
    "$$ h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} $$\n",
    "$$ h(x) = \\sum_{i=0}^{n} x_{i}\\theta_{i}  = \\theta^T x $$\n",
    "(as $x_{0} = 1$ - the intercept term)\n",
    "\n",
    "* Cost \n",
    "\n",
    "The mean squared error (MSE) of an estimator measures the average of the squares of the errors—that is, the average squared difference between the estimated values and the actual value. \n",
    "\n",
    "$$ J(\\theta) = 1/m \\sum_{i=1}^{m} (h(\\theta)^{(i)} - y^{(i)})^2 $$\n",
    "\n",
    "* Gradient \n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = 2/m\\sum_{i=1}^{m}(h(\\theta^{(i)} - y^{(i)}).x_j^{(i)} $$\n",
    "\n",
    "* Update weights with gradients\n",
    "$$ \\theta_j: = \\theta_j -\\alpha . (2/m .\\sum_{i=1}^{m}(h(\\theta^{(i)} - y^{(i)}).x_{j}^{(i)}) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y,\n",
    "                     initial_weights,\n",
    "                     cost_function,\n",
    "                     gradient_cost_function,\n",
    "                     max_iterations,\n",
    "                     threshold,\n",
    "                     learning_rate=0.05):\n",
    "    \n",
    "    w = initial_weights\n",
    "    cost_function_history = cost_function(X, y, w)\n",
    "    delta_w = np.zeros(w.shape)\n",
    "    i = 0\n",
    "    diff = 1.0e10\n",
    "    \n",
    "    while i < max_iterations and diff > threshold:\n",
    "        delta_w = -learning_rate * gradient_cost_function(X, y, w)\n",
    "        w = w + delta_w\n",
    "        \n",
    "        cost_function_history = np.vstack((cost_function_history, \n",
    "                                           cost_function(X, y, w)))\n",
    "        \n",
    "        i+=1\n",
    "        diff = np.absolute(cost_function_history[-1] - cost_function_history[-2])\n",
    "    \n",
    "    return w, cost_function_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[xi] for xi in x])\n",
    "x = np.hstack((np.ones((len(x),1)),x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights (theta1, theta2): [2.57169172 1.5270674 ]\n",
      "Minimum cost function: 3.4122494175518003\n"
     ]
    }
   ],
   "source": [
    "def gradient_mse(x, y, w):\n",
    "    m = len(x)\n",
    "    predicted = x.dot(w)\n",
    "    grad = (2/m) * (predicted-y).dot(x)\n",
    "    return grad\n",
    "\n",
    "def mse(x, y, w):\n",
    "    m = len(x)\n",
    "    predicted = x.dot(w)\n",
    "    mse = (1/m) * (predicted-y).T.dot(predicted-y)\n",
    "    return mse  \n",
    "\n",
    "w_init = np.array([-10, 20])\n",
    "\n",
    "weights, cost_function_history = gradient_descent(x, y, w_init, \n",
    "                                                  mse, gradient_mse,\n",
    "                                                  1000, 0.000000001)\n",
    "print(\"Weights (theta1, theta2):\", weights)\n",
    "print(\"Minimum cost function:\", cost_function_history[-1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliography:\n",
    "\n",
    "Gradient Descent\n",
    "\n",
    "* https://stackabuse.com/gradient-descent-in-python-implementation-and-theory/\n",
    "* https://towardsdatascience.com/gradient-descent-in-python-a0d07285742f\n",
    "* https://colab.research.google.com/gist/sagarmainkar/5cfa33898a303f895da5100472371d91/notebook.ipynb#scrollTo=KdAHnLQKY9ao\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
