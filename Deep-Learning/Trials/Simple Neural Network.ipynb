{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    return 1/(1+np.exp(-t))\n",
    "\n",
    "def sigmoid_derivative(t):\n",
    "    return sigmoid(t) * (1 - sigmoid(t))\n",
    "\n",
    "def mse(y_hat, y):\n",
    "    return np.mean(np.square(y_hat - y))\n",
    "    \n",
    "def mse_derivative(y_hat, y):\n",
    "    return 2*(y_hat - y)\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \n",
    "        self.W1 = np.random.rand(input_size, hidden_size) \n",
    "        self.W2 = np.random.rand(hidden_size, output_size)\n",
    "        \n",
    "        self.b1 = np.zeros((hidden_size, 1))\n",
    "        self.b2 = np.zeros((output_size, 1))\n",
    "        \n",
    "    def feedforward(self, X, sigma):\n",
    "        A0 = X\n",
    "        \n",
    "        self.Z1 = A0.dot(self.W1) + self.b1\n",
    "        self.A1 = sigma(self.Z1)\n",
    "\n",
    "        self.Z2 = self.A1.dot(self.W2) + self.b2\n",
    "        self.A2 = sigma(self.Z2)\n",
    "        \n",
    "        return self.A2\n",
    "        \n",
    "    def backprop(self, X, y, d_sigma, d_loss, alpha=1):\n",
    "        A0 = X\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        d_Z2 = d_loss(self.A2, y) * d_sigma(self.Z2)\n",
    "        \n",
    "        d_W2 = (1/m) * self.A1.T.dot(d_Z2)\n",
    "        d_b2 = (1/m) * np.sum(d_Z2, axis=0, keepdims=True)\n",
    "        \n",
    "        d_Z1 = d_Z2.dot(self.W2.T) * d_sigma(self.A1)\n",
    "        \n",
    "        d_W1 = (1/m) * A0.T.dot(d_Z1)\n",
    "        d_b1 = (1/m) * np.sum(d_Z1, axis=1, keepdims=True)\n",
    "    \n",
    "        self.W1 -= alpha * d_W1\n",
    "        self.W2 -= alpha * d_W2\n",
    "        \n",
    "        self.b1 -= alpha * d_b1\n",
    "        self.b2 -= alpha * d_b2\n",
    "      \n",
    "    def train(self, X, y, sigma, d_sigma, d_loss):\n",
    "        self.feedforward(X, sigma)\n",
    "        self.backprop(X, y, d_sigma, d_loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for iteration # 0\n",
      "\n",
      "Predicted Output: \n",
      "[[0.80612118]\n",
      " [0.83351175]\n",
      " [0.83535902]\n",
      " [0.85561083]]\n",
      "Loss: \n",
      "0.3591815602416715\n",
      "\n",
      "\n",
      "for iteration # 100\n",
      "\n",
      "Predicted Output: \n",
      "[[0.16652085]\n",
      " [0.87263352]\n",
      " [0.87340161]\n",
      " [0.17707881]]\n",
      "Loss: \n",
      "0.022833867799705906\n",
      "\n",
      "\n",
      "for iteration # 200\n",
      "\n",
      "Predicted Output: \n",
      "[[0.08193853]\n",
      " [0.94196288]\n",
      " [0.94204705]\n",
      " [0.08169497]]\n",
      "Loss: \n",
      "0.005028710960775753\n",
      "\n",
      "\n",
      "for iteration # 300\n",
      "\n",
      "Predicted Output: \n",
      "[[0.06087484]\n",
      " [0.95780062]\n",
      " [0.95781764]\n",
      " [0.06007493]]\n",
      "Loss: \n",
      "0.0027187201663977645\n",
      "\n",
      "\n",
      "for iteration # 400\n",
      "\n",
      "Predicted Output: \n",
      "[[0.05063248]\n",
      " [0.96525897]\n",
      " [0.96525867]\n",
      " [0.04983238]]\n",
      "Loss: \n",
      "0.0018652032000555827\n",
      "\n",
      "\n",
      "for iteration # 500\n",
      "\n",
      "Predicted Output: \n",
      "[[0.04433341]\n",
      " [0.96975841]\n",
      " [0.96975199]\n",
      " [0.04360819]]\n",
      "Loss: \n",
      "0.0014241552034286657\n",
      "\n",
      "\n",
      "for iteration # 600\n",
      "\n",
      "Predicted Output: \n",
      "[[0.0399658 ]\n",
      " [0.97283689]\n",
      " [0.97282801]\n",
      " [0.03932048]]\n",
      "Loss: \n",
      "0.0011548790958556116\n",
      "\n",
      "\n",
      "for iteration # 700\n",
      "\n",
      "Predicted Output: \n",
      "[[0.0367087 ]\n",
      " [0.97511008]\n",
      " [0.97510022]\n",
      " [0.03613502]]\n",
      "Loss: \n",
      "0.0009731940560747416\n",
      "\n",
      "\n",
      "for iteration # 800\n",
      "\n",
      "Predicted Output: \n",
      "[[0.03415816]\n",
      " [0.97687669]\n",
      " [0.97686648]\n",
      " [0.03364616]]\n",
      "Loss: \n",
      "0.0008421729114766097\n",
      "\n",
      "\n",
      "for iteration # 900\n",
      "\n",
      "Predicted Output: \n",
      "[[0.03208973]\n",
      " [0.9783008 ]\n",
      " [0.97829054]\n",
      " [0.03163038]]\n",
      "Loss: \n",
      "0.0007430967820691848\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X=np.array(([0,0,1],[0,1,1],[1,0,1],[1,1,1]), dtype=float)\n",
    "y=np.array(([0],[1],[1],[0]), dtype=float)\n",
    "\n",
    "input_size = X.shape[1]\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "NN = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "for i in range(1000): # trains the NN 1,000 times\n",
    "    if i % 100 ==0: \n",
    "        print (\"for iteration # \" + str(i) + \"\\n\")\n",
    "        forward = NN.feedforward(X, sigmoid)\n",
    "        print (\"Predicted Output: \\n\" + str(forward))\n",
    "        print (\"Loss: \\n\" + str(mse(forward, y)))\n",
    "        print (\"\\n\")\n",
    "  \n",
    "    NN.train(X, y, sigmoid, sigmoid_derivative, mse_derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliography:\n",
    "  \n",
    "1. Softmax\n",
    "    * https://deepnotes.io/softmax-crossentropy\n",
    "    * https://aimatters.wordpress.com/2019/06/17/the-softmax-function-derivative/\n",
    "    * https://aimatters.wordpress.com/2020/06/14/derivative-of-softmax-layer/\n",
    "    \n",
    "2. Entropy/ Cross-Entropy\n",
    "    * https://www.youtube.com/watch?v=ErfnhcEV1O8\n",
    "    * https://machinelearningmastery.com/cross-entropy-for-machine-learning/\n",
    "    * https://datascience.stackexchange.com/questions/20296/cross-entropy-loss-explanation\n",
    "\n",
    "3. Neural Network:\n",
    "    * https://cs231n.github.io/neural-networks-case-study/\n",
    "    * https://cs231n.github.io/\n",
    "    * https://github.com/tyz910\n",
    "    * http://cs231n.stanford.edu/handouts/linear-backprop.pdf\n",
    "    \n",
    "4. Numpy axis:\n",
    "    * https://i.stack.imgur.com/Z29Nn.jpg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
